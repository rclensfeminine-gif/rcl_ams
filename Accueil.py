import streamlit as st
import pandas as pd
import os
import json
from google.cloud import firestore
from google.oauth2 import service_account
from datetime import datetime
from sections.menu.menu import custom_sidebar_menu
from sections.constantes import cols_cumul_sum as initial_cols_cumul_sum
from sections.gps.pipeline import recuperer_all_files_gps, recuperer_all_files_gps_match, add_columns_session_rpe
from sections.gps.norme import calculer_toutes_references, calculer_prescription_groupe_auto

# ----------------------------------------------------------------------
# 1. CONFIGURATION DE LA PAGE
# ----------------------------------------------------------------------
# Doit √™tre le premier appel Streamlit
st.set_page_config(
    page_title="AMS RCL F",
    page_icon="‚öΩ", # J'ai ajout√© une ic√¥ne
    layout="wide",
    initial_sidebar_state="expanded",
    # Cette partie masque les √©l√©ments du menu hamburger (trois points)
    menu_items={
        'Get Help': None,
        'Report a bug': None,
        'About': None
    }
)

# ----------------------------------------------------------------------
# 2. INJECTION CSS POUR MASQUER LA NAVIGATION NATIVE
# ----------------------------------------------------------------------
# Ceci masque les √©l√©ments 'Pages' natifs de Streamlit
hide_streamlit_pages_css = """
<style>
/* Masque la navigation de la page native (Home, GPS groupe, etc.) */
section[data-testid="stSidebar"] ul:first-child {
    display: none;
}
/* Masque le titre "Pages" ou "Accueil" juste au-dessus de la liste */
section[data-testid="stSidebar"] div[data-testid="stSidebarNav"] > div:first-child {
    display: none;
}
</style>
"""
st.markdown(hide_streamlit_pages_css, unsafe_allow_html=True)


custom_sidebar_menu()
st.sidebar.caption("Fichiers")

# Etat g√©n√©ral
if 'all_gps_session' not in st.session_state:
    df_gps = recuperer_all_files_gps()
    st.session_state['all_gps_session'] = df_gps #stocker la clef
else:
    df_gps = st.session_state['all_gps_session']
calculer_toutes_references(df_gps)


if 'all_gps_match' not in st.session_state:
    df_gps_match = recuperer_all_files_gps_match()
    st.session_state['all_gps_match'] = df_gps_match
else:
    df_gps_match = st.session_state['all_gps_match']

if 'df_groupe_final' in st.session_state:
    df_groupe_final = st.session_state['df_groupe_final']

if 'df_best_match_all_players' in st.session_state:
    
    df_meilleur_match = st.session_state.get('df_best_match_all_players')
else:
    st.sidebar.write("DataFrame non encore calcul√©.")

# ----------------------------------------------------------------------
# üö® MODIFICATION 1 : CHARGEMENT ET PR√âPARATION DU FICHIER ID (Format Catapult)
# ----------------------------------------------------------------------
try:
    # Fonction d'aide pour normaliser les noms pour la RECHERCHE interne.
    # Nous utilisons PR√âNOM NOM en MAJUSCULES pour la recherche car cela matche
    # le format probable de vos donn√©es GPS/RPE.
    def normalize_catapult_name_for_lookup(name):
        """Convertit 'NOM, Pr√©nom' en 'PR√âNOM NOM' et met tout en MAJUSCULES pour la RECHERCHE."""
        name = str(name).strip()
        if pd.isna(name):
            return ""
            
        name_upper = name.upper()
        
        # Tente de convertir le format 'NOM, Pr√©nom'
        if ',' in name_upper:
            try:
                # S√©pare en utilisant la casse MAJUSCULES
                parts = [p.strip() for p in name_upper.split(',', 1)]
                last = parts[0]
                first = parts[1]
                # Format de RECHERCHE : PR√âNOM NOM en MAJ
                return f"{first} {last}".strip()
            except:
                return name_upper
        else:
            # Si le format est d√©j√† PR√âNOM NOM, retourne juste en MAJUSCULES
            return name_upper

    df_id_joueuses = pd.read_csv('data/ID_joueuses.csv') 
    df_id_joueuses = df_id_joueuses[['Athlete ID', 'Athlete Name', 'Position']].copy()
    
    if 'Athlete Name' in df_id_joueuses.columns:
        # üö® NOUVEAU : Cr√©e une colonne pour la RECHERCHE üö®
        df_id_joueuses['Normalized Name'] = df_id_joueuses['Athlete Name'].apply(normalize_catapult_name_for_lookup)
        
        # Le format original 'Athlete Name' (NOM, Pr√©nom) est conserv√© pour la SORTIE (format Catapult)
    
    st.sidebar.success("Fichier ID Catapult charg√© et noms normalis√©s pour la recherche.")

except FileNotFoundError:
    st.error("Erreur : Le fichier d'identification 'data/ID_joueuses.csv' n'a pas √©t√© trouv√©. Veuillez v√©rifier le chemin.")
    df_id_joueuses = pd.DataFrame() 

###### TEST
# Initialisation du DataFrame complet si ce n'est pas d√©j√† fait
FILE_PATH = "data/rpe.csv"
if not os.path.exists("data"):
    os.makedirs("data")

@st.cache_data
def load_and_merge_rpe(df_base):
    try:
        df_rpe = pd.read_csv(FILE_PATH)
    except (FileNotFoundError, pd.errors.EmptyDataError):
        df_rpe = pd.DataFrame(columns=['Name', 'Date', 'Jour semaine', 'Cardio', 'Muscu', 'Presence'])
    
    # Conversion de la colonne 'Date' dans les deux DataFrames pour √©viter l'erreur de fusion
    df_base['Date'] = pd.to_datetime(df_base['Date'], dayfirst=True)
    df_rpe['Date'] = pd.to_datetime(df_rpe['Date'], dayfirst=True, errors='coerce')
    
    df_merged = pd.merge(df_base, df_rpe, on=['Name', 'Date', 'Jour semaine'], how='left')
    df_merged['Presence'] = df_merged['Presence'].fillna('C')
    return df_merged

DF_REFERENCES_MOYENNES = {
    'J-1': st.session_state.get('ref_j1_cumule', pd.DataFrame()),
    'J-2': st.session_state.get('ref_j2_cumule', pd.DataFrame()),
    'J-3': st.session_state.get('ref_j3_cumule', pd.DataFrame()),
    'J-4': st.session_state.get('ref_j4_cumule', pd.DataFrame()),
}

if 'DF_REFERENCES_MOYENNES' not in st.session_state:
    st.session_state['DF_REFERENCES_MOYENNES'] = DF_REFERENCES_MOYENNES

if 'df_joueuse_rpe_complete' not in st.session_state:
    st.session_state['df_joueuse_rpe_complete'] = load_and_merge_rpe(df_gps.copy())

filtered_df = st.session_state['df_joueuse_rpe_complete']
filtered_df = add_columns_session_rpe(df=filtered_df)
filtered_df['Date'] = pd.to_datetime(filtered_df['Date'],format='%Y-%m-%d')
filtered_df['Date'] = filtered_df['Date'].dt.strftime('%Y-%m-%d')

# ----------------------------------------------------------------------
# üö® MODIFICATION 2 : NETTOYAGE DES COLONNES IND√âSIRABLES
# ----------------------------------------------------------------------
cols_a_retirer_finales = ['Total Time', 'S. Cardio', 'S. Muscu']

filtered_df = filtered_df.drop(
    columns=[col for col in cols_a_retirer_finales if col in filtered_df.columns], 
    errors='ignore'
)

# ----------------------------------------------------------------------
# üö® MODIFICATION 3 : D√âFINITION DE LA LISTE DE COLONNES FINALES (cols_cumul_sum)
# ----------------------------------------------------------------------
# BUT : Retirer uniquement les colonnes inutiles (Total Time, S. Cardio, S. Muscu)
# ET GARANTIR QUE 'High Speed Distance' RESTE POUR √äTRE UTILIS√âE ET RECALCUL√âE.
cols_a_retirer_temporaires = ['Total Time', 'S. Cardio', 'S. Muscu', 'Sprint(m)'] # On enl√®ve 'Sprint(m)' au cas o√π il existerait
cols_cumul_sum = [
    col for col in initial_cols_cumul_sum 
    if col not in cols_a_retirer_temporaires
]

# Note: Si 'High Speed Distance' √©tait dans la liste initiale, il y reste. C'est ce que nous voulons.
# Nous n'avons plus besoin de la ligne qui ajoutait 'Sprint(m)' car nous utilisons HSD.


if not filtered_df.empty:
    # Correction: On s'assure que toutes les colonnes num√©riques sont correctement typ√©es AVANT tout calcul
    # On it√®re sur toutes les colonnes de cols_cumul_sum (qui inclut HSD) plus les colonnes SPR et SPR+ pour √™tre s√ªr
    for col in cols_cumul_sum + ['SPR', 'SPR+']: 
        if col in filtered_df.columns:
            filtered_df[col] = pd.to_numeric(filtered_df[col], errors='coerce')

st.dataframe(filtered_df)

# --- üéØ S√âLECTEUR DE DATE ET D√âFINITION DE LA CIBLE üéØ ---

# 1. D√©finir la date du jour comme valeur par d√©faut
date_defaut = datetime.now().date()

# On assure que la colonne est de type datetime pour le .max()
if 'Date' in filtered_df.columns:
     # S'assurer que le format est bien datetime (d√©j√† fait dans load_and_merge_rpe, mais s√©curis√© ici)
     filtered_df['Date'] = pd.to_datetime(filtered_df['Date'], errors='coerce') 
     filtered_df.dropna(subset=['Date'], inplace=True)


# --- üéØ S√âLECTEUR DE DATE ET D√âFINITION DE LA CIBLE üéØ ---

# 2. Trouver la date la plus r√©cente dans les donn√©es pour l'adapter (Maintenant que c'est un type datetime)
if not filtered_df.empty:
    # La colonne est maintenant de type datetime, on peut utiliser .date() sans erreur
    date_max = filtered_df['Date'].max().date() 
    
    # Si la date max est pass√©e, utiliser la date max comme d√©faut
    # Note: On compare l'objet date_max (date) √† date_defaut (date)
    if date_max > date_defaut: 
         date_defaut = date_max

date_selectionnee = st.date_input(
    "S√©lectionnez la **date d'analyse** du microcycle :",
    value=date_defaut
    # Si vous voulez analyser le futur, retirez max_value=datetime.now().date()
)

# 3. Convertir la date s√©lectionn√©e en format datetime pour la comparaison
date_analyse_cible = pd.to_datetime(date_selectionnee)

# ----------------------------------------------------------------------
# üö® MODIFICATION 5 : APPEL DE LA FONCTION DE GROUPE AVEC LE FICHIER ID
# ----------------------------------------------------------------------
st.subheader("--- D√©bogage des prescriptions (Les avertissements ci-dessus indiquent les √©checs) ---")
df_prescriptions_groupe, jour_reference_calc, jour_a_ajuster_calc = calculer_prescription_groupe_auto(
    df_rpe_complet=filtered_df, # Votre DataFrame GPS/RPE complet
    DF_REFERENCES_MOYENNES=DF_REFERENCES_MOYENNES, # Votre dictionnaire de r√©f√©rence
    cols_cumul_sum=cols_cumul_sum, # Votre liste de colonnes m√©triques mises √† jour
    date_analyse_cible=date_analyse_cible,
    df_id_joueuses=df_id_joueuses # üëà NOUVEAU PARAM√àTRE CRITIQUE
)

# ----------------------------------------------------------------------
# üö® MODIFICATION 6 : AFFICHAGE FINAL (Format Catapult)
# ----------------------------------------------------------------------
if not df_prescriptions_groupe.empty:
    st.header("üìã Prescription de Groupe : S√©ance Corrig√©e")
    st.subheader(f"Charge Brute Corrig√©e Recommand√©e pour **{jour_a_ajuster_calc}** (√âvaluation bas√©e sur **{jour_reference_calc}**)")
    
    # L'affichage utilise le DataFrame format√© Catapult qui a d√©j√† les bonnes colonnes
    st.dataframe(df_prescriptions_groupe.style
            .format(precision=2),
        use_container_width=True, # Important pour remplir la colonne
        hide_index=True
    )

    df_prescriptions_seance = df_prescriptions_groupe.drop(['Athlete ID', 'Position'], axis=1)
    
    cols_to_avg = [col for col in df_prescriptions_seance.columns 
        if col not in ['Athlete Name']]
    
    df_prescriptions_seance = df_prescriptions_seance[cols_to_avg].mean().to_frame().T
    st.dataframe(df_prescriptions_seance.style
            .format(precision=2),
        use_container_width=True, # Important pour remplir la colonne
        hide_index=True
    )

else:
    st.error("‚ùå √âchec de la prescription : Aucune joueuse n'a pu √™tre calcul√©e. Veuillez v√©rifier les avertissements ci-dessus.")

#st.dataframe(df_meilleur_match)

def init_connection():
    """Initialise la connexion √† Firestore en utilisant les Secrets de Streamlit."""
    try:
        # On r√©cup√®re les secrets au format dictionnaire
        key_dict = dict(st.secrets["firestore"])
        
        # On cr√©e les credentials
        creds = service_account.Credentials.from_service_account_info(key_dict)
        
        # On initialise le client Firestore
        return firestore.Client(credentials=creds, project=key_dict['project_id'])
    except Exception as e:
        st.error(f"Erreur d'initialisation : {e}")
        return None

st.title("üöÄ Test de connexion Firestore")

db = init_connection()

if db:
    st.success("Connexion r√©ussie aux Secrets et √† Google Cloud !")
    
    # Test de lecture/√©criture simple
    st.subheader("V√©rification des donn√©es")
    
    # Bouton pour tester l'√©criture
    if st.button("√âcrire un document de test"):
        doc_ref = db.collection("test_collection").document("test_doc")
        doc_ref.set({
            "status": "Connect√© !",
            "message": "Bravo, tes secrets fonctionnent parfaitement.",
            "timestamp": firestore.SERVER_TIMESTAMP
        })
        st.info("Document 'test_doc' cr√©√© dans la collection 'test_collection'.")

    # Affichage des documents existants (si disponibles)
    try:
        docs = db.collection("test_collection").stream()
        st.write("Documents trouv√©s dans 'test_collection' :")
        for doc in docs:
            st.json(doc.to_dict())
    except Exception as e:
        st.warning(f"Impossible de lire la collection (elle est peut-√™tre vide) : {e}")
else:
    st.error("La connexion a √©chou√©. V√©rifie le format de ta 'private_key' dans les Secrets.")
    st.info("Astuce : La cl√© doit commencer par `-----BEGIN PRIVATE KEY-----` et finir par `-----END PRIVATE KEY-----\\n`.")